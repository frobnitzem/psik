#!{{rc}}
{{#job.name}}#SBATCH --job-name="{{.}}"
{{/job.name}}
#SBATCH --chdir="{{job.directory}}"
{{#job.inherit_environment}}#SBATCH --export=ALL
{{/job.inherit_environment}}
{{^job.inherit_environment}}#SBATCH --export=NONE
{{/job.inherit_environment}}
{{#job.environment}}#SBATCH --export={{name}}={{value}}
{{/job.environment}}

{{#job.resources}}
{{#duration}}#SBATCH --time={{.}}
{{/duration}}
{{#node_count}}#SBATCH --nodes={{.}}
{{/node_count}}
{{#process_count}}#SBATCH --ntasks={{.}}
{{/process_count}}
{{#processes_per_node}}#SBATCH --ntasks-per-node={{.}}
{{/processes_per_node}}
{{#gpu_cores_per_process}}#SBATCH --gpus-per-task={{.}}
{{/gpu_cores_per_process}}
{{!SLURM has different meanings for 'CPU', depending on the exact allocation plugin being
   used. The default plugin uses the literal meaning for 'CPU' - physical processor. The
   consumable resource allocation plugin, on the other hand, equates 'CPU' to a thread (if
   hyperthreaded CPUs are used) or CPU core (for non-hyperthreaded CPUs).}}
{{#cpu_cores_per_process}}#SBATCH --cpus-per-task={{.}}
{{/cpu_cores_per_process}}
{{#exclusive_node_use}}#SBATCH --exclusive
{{/exclusive_node_use}}
{{/job.resources}}

{{#job.backend}}
{{#queue_name}}#SBATCH --partition="{{.}}"
{{/queue_name}}
{{#project_name}}#SBATCH --account="{{.}}"
{{/project_name}}
{{#reservation_id}}#SBATCH --reservation="{{.}}"
{{/reservation_id}}
{{#attributes}}#SBATCH {{key}}="{{value}}"
{{/attributes}}
{{/job.backend}}
#SBATCH -e /dev/null
#SBATCH -o /dev/null

# setup fixed job env variables
mpirun = srun
nodes  = $SLURM_JOB_NUM_NODES
jobndx = $1
jobid  = $SLURM_JOB_ID
base   = '{{base}}'
# nodelist = ...

{{>job_body}}
