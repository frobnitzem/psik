job:
{{#job.name}}
  name: "{{.}}"
{{/job.name}}
  current_working_directory: "{{job.directory}}"
  environment:
  - USING_PSIK: true
{{#job.environment}}
  - "{{name}}": "{{value}}"
{{/job.environment}}

{{#job.resources}}
{{#duration}}
  time_limit:
    number: "{{.}}"
    set: true
{{/duration}}
{{#node_count}}
  nodes: "{{.}}"
{{/node_count}}
{{#process_count}}
  ntasks: "{{.}}"
{{/process_count}}
{{#processes_per_node}}
  tasks_per_node: "{{.}}"
{{/processes_per_node}}
{{!not present? #gpu_cores_per_process
  gpus_per_task=...
 /gpu_cores_per_process}}

{{!SLURM has different meanings for 'CPU', depending on the exact allocation plugin being
   used. The default plugin uses the literal meaning for 'CPU' - physical processor. The
   consumable resource allocation plugin, on the other hand, equates 'CPU' to a thread (if
   hyperthreaded CPUs are used) or CPU core (for non-hyperthreaded CPUs).}}
{{#cpu_cores_per_process}}
  cpus_per_task: "{{.}}"
{{/cpu_cores_per_process}}
{{/job.resources}}

{{#job.backend}}
{{#queue_name}}
  partition: "{{.}}"
{{/queue_name}}
{{#project_name}}
  account: "{{.}}"
{{/project_name}}
{{#reservation_id}}
  reservation: "{{.}}"
{{/reservation_id}}
{{/job.backend}}
  standard_input: /dev/null
  standard_output: /dev/null
  standard_error: /dev/null
  script: |
    #!/usr/bin/env rc
    # setup fixed job env variables
    mpirun = srun
    nodes  = $SLURM_JOB_NUM_NODES
    jobndx = %(jobndx)d
    jobid  = $SLURM_JOB_ID
    base   = '{{base}}'

    {{>job_body}}
